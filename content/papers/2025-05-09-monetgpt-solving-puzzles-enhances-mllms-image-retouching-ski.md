---
title: "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills"
date: 2025-05-09
authors:
  - "Niladri Shekhar Dutt"
  - "Duygu Ceylan"
  - "Niloy J. Mitra"
source: "arXiv"
arxiv_url: "https://arxiv.org/abs/2505.06176"
pdf_url: "https://arxiv.org/pdf/2505.06176"
one_line_summary: "MonetGPT demonstrates that training MLLMs on specially-designed visual puzzles enables operation-aware procedural image retouching with superior identity preservation and explainability compared to generative methods."
one_line_summary_zh: "MonetGPT é€éè¨“ç·´ MLLM è§£æ±ºç‰¹åˆ¥è¨­è¨ˆçš„è¦–è¦ºè¬é¡Œï¼Œå¯¦ç¾æ“ä½œæ„ŸçŸ¥çš„ç¨‹åºåŒ–åœ–åƒä¿®é£¾ï¼Œç›¸æ¯”ç”Ÿæˆå¼æ–¹æ³•å…·æœ‰æ›´å„ªè¶Šçš„èº«ä»½ä¿ç•™å’Œå¯è§£é‡‹æ€§ã€‚"
date_added: 2026-02-10
topics: ["Agentic Pipeline"]
tags: []
---

<div class="lang-en">

## Key Contributions

- **Operation-Aware MLLM for Procedural Image Retouching**: This paper presents **MonetGPT**, the first framework to fine-tune multimodal large language models (MLLMs) to understand and plan procedural image retouching operations with precise parameter estimation. Unlike generative editing methods that regenerate pixels (causing identity loss), MonetGPT works with a curated library of 33 pre-defined procedural operations organized in three stages (lighting, color/temperature, color-specific adjustments). This preserves object identity, resolution, and provides full user control through interpretable, non-destructive edits compatible with high-resolution 16-bit images.

- **Visual Puzzle-Based Skill Learning Strategy**: The paper introduces a novel training approach that teaches MLLMs image operation awareness through three specially-designed visual puzzles rather than direct supervised learning on paired data. Puzzle A teaches individual operation effects (given before/after images, predict the operation and value), Puzzle B teaches aesthetic judgment and optimal parameter ranges, and Puzzle C teaches comprehensive editing plan generation. This design addresses the MLLM's fundamental lack of understanding about what image operations do, enabling learning from limited expert data (~7k samples for Puzzle A, ~5k for Puzzle B, ~13k for Puzzle C).

- **Reasoning-Grounded Dataset Synthesis**: To leverage puzzle-based learning, the authors synthesize reasoning explanations by grounding a pre-trained LLM (Gemini 2.0 Flash) on actual visual adjustments rather than allowing hallucination. For each puzzle instance, the model generates structured reasoning in the form of <Adjustment, Issue, Solution> triplets that explain how each operation addresses specific visual problems. This reasoning serves as both training signal and inference pathwayâ€”during inference, the MLLM first generates high-level reasoning about image issues, then uses this reasoning to regress precise parameter values, effectively using reasoning as a bridge from intent to numerical adjustments.

- **Staged Procedural Pipeline with User Interactivity**: MonetGPT applies edits in three stages (lighting â†’ color adjustments â†’ color-specific fine-tuning), enabling multiple advantages: (1) invertibility of operations within categories to synthesize training data reliably, (2) reduced complexity compared to simultaneous multi-operation planning, (3) clearer reasoning about which operation causes which visual effect. Critically, the autoregressive nature of MLLMs enables users to edit and override the plan at any stage, with subsequent stages automatically adaptingâ€”users can modify any proposed adjustment and have the system re-plan downstream operations accordingly.

- **Comprehensive Evaluation Against Generative and Procedural Baselines**: The paper provides extensive evaluation comparing against: RL-based methods (Exposure), generative approaches (MGIE, InstructPix2Pix), unpaired enhancement methods, direct MLLM regression variants, Gemini 2.0 with chain-of-thought, and commercial software (Google Photos). Evaluation includes quantitative metrics (SSIM, LPIPS, PSNR, histogram intersection on 400 Adobe5k images), qualitative user studies with both novice (n=15) and expert (n=10) raters, and ablation studies demonstrating the necessity of puzzle-based training versus direct regression.

- **Practical Library Implementation with Simplified Parameter Space**: Rather than using existing complicated tools like GIMP, the authors developed a Python library with 33 modular adjustment operations where each operation is controlled by a single master parameter (with sub-parameters either fixed or derived). Operations follow a perceptually linear [-100, +100] scale, enabling the fine-tuned MLLM to generate JSON-format edit plans that directly execute without additional coding, making the approach practical for end-users while maintaining interpretability.

## Core Insights

- **MLLMs Lack Operational Understanding Without Explicit Training**: The paper demonstrates a critical insight that pre-trained MLLMs, despite their strong visual understanding, have fundamentally poor internal models of what image operations do and how they affect images. Direct fine-tuning on paired retouching data (the "MLLM Regression" baseline) fails significantly because the model cannot connect visual changes to specific operations. This explains why naÃ¯ve approaches like prompting Gemini 2.0 with a library of operations perform poorly. The solutionâ€”training on visual puzzles that explicitly teach operation semanticsâ€”reveals that operation-awareness is a learnable skill that transfers to accurate planning.

- **Reasoning as a Pathway for Parameter Regression**: Rather than directly regressing numerical parameters from images (which causes overfitting with limited data), MonetGPT uses chain-of-thought reasoning as an intermediate representation that bridges high-level image analysis to precise parameter values. During inference, the model first generates semantic descriptions of problems and solutions, then translates these into numerical adjustments. This two-stage process (reasoning â†’ regression) significantly outperforms direct regression baselines, suggesting that LLMs are better suited to semantic reasoning about editing intent than numerical regression from visual features.

- **Invertibility Enables Synthetic Data Generation at Scale**: By restricting modifications within parameter categories that remain invertible (e.g., modifying only lighting operations, then only color operations), the authors can procedurally generate training puzzles from expert-edited images without requiring paired source-target data. They sample an expert-edited image ğ¼ğ‘‹, apply random operation perturbations to create degraded versions ğ¼ğ‘†, then use grounded reasoning generation to label why each operation helps restore quality. This unpaired approach is criticalâ€”the dataset is synthesized entirely from expert edits without ever seeing the original source images during training, yet generalizes to unseen sources.

- **Staged Planning Reduces State Space and Improves Reasoning Quality**: Breaking the planning task into three stages (lighting â†’ color/temperature â†’ color-specific) rather than jointly predicting all operations dramatically simplifies the problem. Figure 7 demonstrates that direct regression produces nearly identical parameter values across different input conditions, while staged MonetGPT produces appropriately varied adjustments. This staging enables clearer causal reasoning about which operations address which visual issuesâ€”humans cannot easily explain why 15 simultaneous operations are needed, but can clearly articulate why blacks need lifting and highlights need reduction in specific lighting scenarios.

- **Operation-Awareness Transfers to Diverse Aesthetic Preferences**: Figure 7 (right panel) shows that the same operation-aware MLLM can generate different editing plans based on style tags ("retro," "balanced," "vibrant") by simply conditioning the prompt differently. This flexibility would be impossible with fixed parameter regression. The underlying operation understanding allows the model to adapt its adjustments based on stylistic intent, suggesting that teaching machines about operation semantics unlocks greater adaptability than teaching specific editing patterns.

- **Identity Preservation and Explainability Align with User Preferences**: User studies show strong preference for MonetGPT (preferred by 60-70% of both novice and expert users vs. Exposure or MGIE), and this preference correlates with identity preservation and explainability. Experts appreciated the interpretable nature of adjustments and could suggest minor refinements like "increase skin tone saturation," indicating they understand and can override the system. In contrast, generative methods that preserve or improve metrics may still fail user preferences due to identity loss, highlighting that procedural approaches better serve professional workflows despite slightly lower PSNR values.

## Key Data & Results

| Method | SSIM â†‘ | LPIPS â†“ | PSNR â†‘ | Histogram â†‘ |
|--------|--------|--------|--------|------------|
| Exposure [Hu et al. 2018] | 0.63 | 0.14 | 15.12 | 47.21 |
| Unpaired [Kosugi et al. 2020] | 0.83 | 0.12 | 21.73 | 83.98 |
| RSFNet [Ouyang et al. 2023] | 0.88 | 0.08 | 21.85 | 80.26 |
| InstructPix2Pix [Brooks et al. 2023] | 0.61 | 0.22 | 16.99 | 73.90 |
| MGIE [Fu et al. 2024] | 0.74 | 0.08 | 22.94 | 79.95 |
| Gemini CoT + Library | 0.80 | 0.14 | 17.83 | 63.71 |
| GooglePhotos | 0.90 | 0.06 | 25.86 | 86.47 |
| MLLM Regression | 0.84 | 0.10 | 20.89 | 82.05 |
| **MonetGPT (ours)** | **0.90** | **0.07** | **23.75** | **79.50** |

**Dataset & Evaluation Setup**: Trained on PPR10K (expert A only, unpaired setup with synthetic puzzles), tested on 400 images from Adobe5k for generalization. Metrics include perceptual similarity (SSIM, LPIPS), pixel fidelity (PSNR), and histogram intersection across contrast/luminance/saturation. User study on 50 Adobe5k and Reddit images with 15 novice and 10 expert raters (200 binary preference questions).

- **Quantitative Performance**: MonetGPT achieves competitive or leading performance on most metricsâ€”tied with GooglePhotos on SSIM (0.90), comparable LPIPS (0.07 vs. 0.06), exceeding most open-source baselines on PSNR (23.75). The histogram metric (79.50) indicates good distribution matching, though slightly below GooglePhotos (86.47) and Unpaired (83.98). On LPIPS, MonetGPT (0.07) matches the best performer (RSFNet 0.08, MGIE 0.08) while using an interpretable procedural approach rather than end-to-end generation.

- **User Preference Strong Across Expertise Levels**: In head-to-head comparisons, MonetGPT was preferred by approximately 65-70% of both novice and expert users against Exposure and MGIE baselines. Figure 8 clearly demonstrates consistent preference across user groups, with experts providing constructive feedback (e.g., "increase skin saturation," "better curve emphasis") that indicates they understand and trust the proposed adjustments. This preference gap is notable because it contradicts purely metric-based rankingâ€”methods with higher PSNR (GooglePhotos 25.86 vs. MonetGPT 23.75) were not consistently preferred in user studies.

- **Ablation: Direct Regression Severely Underperforms**: The MLLM Regression baseline (same model, architecture, training data but trained to directly predict parameters) achieves SSIM 0.84, LPIPS 0.10, PSNR 20.89â€”substantially below MonetGPT on SSIM (0.90) and PSNR. Figure 10 shows that the regression baseline suffers from severe overfitting: it predicts nearly identical parameter values across 100 different images, failing to capture the full range of adjustments. In contrast, MonetGPT's violin plots show it appropriately uses the full parameter range [-100, +100], confirming that reasoning-based planning enables greater diversity and adaptability.

- **Computational Efficiency Trade-offs**: Full staged pipeline inference requires ~25 seconds on RTX 4090, compared to ~10 seconds for direct regression and ~2 seconds for Exposure. While slower than RL-based methods, this is acceptable for post-processing workflows. Training takes ~8 hours on H100 GPU for puzzle-based fine-tuning vs. 2.5 hours for regression, representing a 3.2Ã— increase. The cost is justified by substantially better generalization and identity preservation compared to alternatives.

- **Generalization to Out-of-Domain Images**: Training only on PPR10K (mostly portraits) but evaluating on diverse Adobe5k images shows reasonable generalization. The histogram intersection metric (79.50) indicates the predicted distributions roughly match expert targets despite domain shift. However, comparison against GooglePhotos on the same evaluation setup suggests room for improvementâ€”GooglePhotos likely leverages larger training datasets that MonetGPT was explicitly constrained to avoid dependence on massive proprietary data.

## Strengths

- **Novel and Well-Motivated Problem Formulation**: The paper addresses a genuine gap between powerful generative editing (which loses identity) and traditional procedural editing (which is difficult to plan). The framing of procedural image retouching as an MLLM reasoning task is novel, and the observation that MLLMs lack operational understanding is insightful and well-evidenced. The approach is grounded in human learning theory (skill learning through puzzle-solving), lending credibility to the design choices. The problem has clear practical value for professional workflows where editability and interpretability are non-negotiable.

- **Innovative Training Strategy via Visual Puzzles**: The three-puzzle framework is creative and well-designed. Rather than forcing the model to learn from limited paired data, the authors cleverly decompose learning into semantic units: understanding individual operations (Puzzle A), aesthetic judgment (Puzzle B), and planning (Puzzle C). The progression is pedagogically sound, and the use of grounded reasoning generation to prevent hallucination is technically sophisticated. This approach is generalizable to other domains requiring operation understanding and could influence future work on skill learning in multimodal models.

- **Comprehensive Experimental Evaluation**: The paper compares against diverse baselines spanning RL-based (Exposure), neural regression (RSFNet), generative (MGIE, InstructPix2Pix), and commercial solutions (GooglePhotos). Evaluation includes multiple dimensions: quantitative metrics (SSIM, LPIPS, PSNR, histogram), user studies with both experts and novices, ablations demonstrating necessity of puzzle-based training, and qualitative analysis with expert feedback. The supplementary material appears extensive (referenced repeatedly). User studies with 25 total raters is reasonable for this domain.

- **Practical Implementation and Reproducibility**: The authors promise to release code, data, models, and supplementary results. The paper provides sufficient implementation details: Qwen-VL-7B fine-tuned with DoRA adapters, specific hyperparameters (learning rate 1e-4, rank 256, single epoch), dataset sizes (7k, 5k, 13k samples), and library specifications (33 operations). The decision to develop a simplified Python library rather than relying on GIMP shows attention to practical deployment. The JSON-based output format makes integration straightforward.

- **Explainability and User Control**: Unlike black-box generative methods, MonetGPT provides human-readable reasoning for each adjustment and enables per-stage editing. Figure 9 demonstrates autoregressive editing where users can modify proposed plans at any stage and have the system re-plan downstream operationsâ€”this is a significant usability advantage. The <Adjustment, Issue, Solution> structured reasoning format is interpretable and actionable, enabling domain experts to verify and refine suggestions.

- **Thoughtful Design Choices with Clear Justification**: The staged pipeline design (Section 4.3) is justified by three considerations: invertibility for data synthesis, reduced complexity, and clearer reasoning. The choice of Qwen-VL over potentially larger models, the quantization of parameters to [-100, +100] for easier tokenization, and the structure of the reasoning dataset all show careful consideration. The paper explains not just WHAT they did but WHY, which aids reproducibility and future work.

## Weaknesses

- **Limited Training Data Diversity and Potential Domain Shift**: The model is trained on only PPR10K expert A with ~25k synthesized puzzle samples total, then evaluated on Adobe5k. PPR10K is predominantly portrait images, while Adobe5k contains more diverse scenes. While the paper claims generalization, quantitative results reveal the gap: histogram intersection (79.50) is notably lower than GooglePhotos (86.47) and several baselines. The paper does not provide cross-domain ablations (e.g., training on both PPR10K experts, evaluating on PPR10K-held-out) to isolate the impact of domain shift vs. the method itself. The strong performance of GooglePhotos suggests that larger, more diverse training data may be critical, but this limitation is not deeply analyzed.

- **Unclear Puzzle Generation and Potential Data Leakage**: The paper states puzzles are generated "synthetically" by applying random operations to expert images, but critical details are missing. How are operation parameters sampled? What is the distribution? For Puzzle B, the paper assumes "any sufficiently large adjustment made to ğ¼ğ‘‹ degrades the image"â€”this is a strong assumption not validated. How are the four perturbation levels chosen in Puzzle B? Is there a principled distribution, or is it ad-hoc? The test images come from a different dataset (Adobe5k vs. PPR10K), but potential data leakage through the synthetic puzzles generated from expert images is not discussed. If Adobe5k was used to generate puzzles, results would be inflated.

- **Insufficient Analysis of Failure Cases and Limitations**: The paper presents mostly successful examples but provides limited discussion of when and why the method fails. Figure 6 shows qualitative comparisons but doesn't highlight failure modes. What types of images does MonetGPT struggle with? The paper mentions experts suggested "minor refinements" but doesn't quantify how often these were needed or how significant they were. The inference time (25 sec) is substantially slower than Exposure (2 sec)â€”for professional workflows, is this acceptable? The paper doesn't discuss this trade-off. Additionally, the method is restricted to the 33 operations in the library; how does it behave when presented with images that would benefit from operations outside this set?

- **Weak Direct Comparison with Gemini 2.0 and Limited Exploration of Prompting**: The "Gemini+Library CoT" baseline is crucial for isolating the benefit of puzzle-based fine-tuning, yet it seems underexplored. The paper mentions three-stage prompting with library names and operation categories but doesn't provide the full prompt in the main text (only references supplementary). With the right prompts, sophisticated models like Gemini 2.0 might perform comparably. The paper shows qualitative examples where Gemini produces over-exposed images, but no systematic analysis of why the puzzle-based approach is fundamentally superior to better prompting strategies. This is a critical baseline for justifying the fine-tuning effort.

- **Missing Quantitative Analysis of Reasoning Quality**: While reasoning generation is central to the approach, the paper provides no quantitative evaluation of reasoning quality. Are the <Adjustment, Issue, Solution> triplets accurate? How do humans rate the explanations? Does the quality of reasoning correlate with edit quality? The paper assumes grounding on actual adjustments prevents hallucination, but provides no evidence. A quantitative evaluation of reasoningâ€”e.g., through expert human rating or information overlap with ground-truth adjustmentsâ€”would strengthen claims about the approach's interpretability. Figure 1 shows an example of good reasoning, but is this representative?

- **Limited Exploration of Alternative Design Choices**: The paper commits to specific choices without thoroughly justifying alternatives. Why three stages rather than two or four? Why Puzzle C's assumption that perturbing expert edits produces worse images? Why fine-tune Qwen-VL-7B rather than larger models like Gemini 2.0 or GPT-4V? The paper mentions using Gemini 2.0 Flash for reasoning generation but doesn't fine-tune Gemini itselfâ€”why? The computational cost of 8 hours training suggests a small model was chosen partly for efficiency, but this trade-off is not explicit. Ablations on these design choices would strengthen the work significantly.

- **Evaluation Metrics May Not Fully Capture Perceptual Quality**: While the user study is valuable, some concerns remain. PSNR/SSIM are included but are known to correlate poorly with human perception of image qualityâ€”the paper relies more on user studies, which is good but limits reproducibility and comparison with future work. The histogram metric measures distribution matching but not perceptual realism. No perceptual quality metrics (e.g., NIQE, BRISQUE) are reported. The "highest score against any expert" evaluation rule (taking max over 5 Adobe5k experts) may inflate resultsâ€”averaging across experts might be fairer for methods not specifically trained on their style. The paper should justify this choice and report both variants.

## Research Directions

- **Multi-Expert Adaptive Fine-Tuning with Style Transfer**: Extend MonetGPT to fine-tune on multiple expert editors simultaneously (e.g., all 5 experts in Adobe5k) rather than a single expert, then learn a style adapter that allows the model to dynamically shift between editing styles based on user preference. This could be achieved by: (1) training the base model on multi-expert puzzles with expert labels, (2) introducing lightweight style-specific LoRA adapters, (3) enabling users to specify desired style as input. This would address the current limitation of single-expert bias and enable the model to capture the diversity of professional editing preferences, creating a more generalizable and personalized system. The approach would be highly relevant for practical deployment where different users have different aesthetic preferences.

- **Joint Learning of Operation Parameters and Sampling Strategies**: Investigate whether the model can learn not just parameter values but also which operations to apply (sparse operation selection). Current approach always outputs all parameters in JSON. Develop a variant where the model learns to decide which operations are necessary and which can be skipped, reducing the parameter space further. This could involve: (1) adding a learned gating mechanism to determine operation necessity, (2) training with a sparsity-inducing loss, (3) evaluating whether sparse plans are preferred by users. This research would enable better transfer to images that require fundamentally different editing strategies and reduce unnecessary operations.

- **Hierarchical Planning with User-in-the-Loop Feedback**: Extend the autoregressive editing capability (Figure 9) into a full interactive system where users provide intermediate feedback at each stage, and the model refines subsequent stage planning based on this feedback. Develop a reinforcement learning framework where: (1) user acceptance/rejection of intermediate edits provides reward signals, (2) the model learns to predict user preferences through interaction, (3) the system improves editing plans based on accumulated feedback. This would require collecting user interaction datasets and designing appropriate RL objectives. The result would be a copilot-style system that learns from users during deployment, relevant for both professional and amateur workflows.

- **Cross-Domain Generalization and Automatic Operation Discovery**: Currently, MonetGPT is restricted to 33 predefined operations. Investigate whether the operation-aware learning mechanism generalizes to new domains and image types (e.g., video, medical images) by learning to discover appropriate operations from task-specific image pairs. Approach: (1) provide the model with a library of differentiable operations for the new domain, (2) use the puzzle-based training strategy with domain-specific expert edits, (3) evaluate generalization. This would demonstrate the universality of the puzzle-learning paradigm and enable application to specialized domains like medical image enhancement, scientific visualization, or video retouching where expert guidelines differ substantially from standard photography.

- **Theoretical Analysis of Operation-Awareness and Transfer Learning**: Conduct deeper investigation into why puzzle-based training is effective through: (1) analyzing what representations the model learns (probing studies to identify operation-specific features), (2) studying transfer from puzzles to planning (do improvements in Puzzle A directly predict improvements in editing quality?), (3) measuring semantic similarity between learned operation representations. This could yield theoretical insights into how multimodal models learn about abstract concepts (operations) vs. perceptual features (image appearance), and inform design of better training objectives. Such analysis could influence how we train LLMs for other domains requiring systematic understanding of abstract tools/operations.

- **Evaluation Framework for Explainable Image Editing**: Develop comprehensive benchmarks and evaluation protocols specifically for explainable image editing. Current work uses standard metrics (SSIM, LPIPS) borrowed from unconditional generation, which may not capture explainability quality. Propose metrics that measure: (1) reasoning accuracy (how well do explanations match actual adjustments), (2) reasoning actionability (can users refine suggestions based on explanations), (3) plan coherence (do operations address complementary issues or redundantly target the same aspect). Implement a user study protocol with quantitative rating scales rather than binary preferences. Such a framework would enable fair evaluation of future explainable editing methods and could become a standard benchmark.

- **Few-Shot Adaptation and Rapid Style Customization**: Investigate whether the operation-aware model can rapidly adapt to new expert styles with few examples (e.g., 5-10 edited images). Design a meta-learning approach where: (1) the base model trained on puzzles serves as initialization, (2) fine-tune with just a few expert edits using LoRA or similar efficient adaptation, (3) evaluate whether the adapted model captures the expert's style. This is practically valuableâ€”photographers often have distinctive styles, and rapid customization would enable personalized retouching. The approach would also test the generality of operation-aware representations: if they truly capture fundamental principles of image editing, adaptation should be sample-efficient.

</div>

<div class="lang-zh" style="display:none;">

## ä¸»è¦è²¢ç»

- **é‡å°ç¨‹åºåŒ–åœ–åƒä¿®é£¾çš„æ“ä½œæ„ŸçŸ¥ MLLM**ï¼šæœ¬æ–‡æå‡º **MonetGPT**ï¼Œé€™æ˜¯é¦–å€‹å¾®èª¿å¤šæ¨¡æ…‹å¤§èªè¨€æ¨¡å‹ (MLLM) ä»¥ç†è§£å’Œè¦åŠƒç¨‹åºåŒ–åœ–åƒä¿®é£¾æ“ä½œä¸¦é€²è¡Œç²¾ç¢ºåƒæ•¸ä¼°è¨ˆçš„æ¡†æ¶ã€‚èˆ‡é‡æ–°ç”Ÿæˆåƒç´ çš„ç”Ÿæˆå¼ç·¨è¼¯æ–¹æ³•ï¼ˆå°è‡´èº«ä»½ä¸Ÿå¤±ï¼‰ä¸åŒï¼ŒMonetGPT èˆ‡ç²¾é¸çš„ 33 å€‹é å®šç¾©ç¨‹åºåŒ–æ“ä½œåº«å”ä½œï¼Œåˆ†ä¸‰å€‹éšæ®µçµ„ç¹”ï¼ˆå…‰ç·šã€è‰²å½©/æº«åº¦ã€ç‰¹å®šè‰²å½©èª¿æ•´ï¼‰ã€‚é€™ç¨®æ–¹æ³•ä¿ç•™äº†ç‰©é«”èº«ä»½ã€åˆ†è¾¨ç‡ï¼Œä¸¦é€šéå¯è§£é‡‹çš„éç ´å£æ€§ç·¨è¼¯æä¾›å®Œæ•´çš„ä½¿ç”¨è€…æ§åˆ¶ï¼Œç›¸å®¹æ–¼é«˜åˆ†è¾¨ç‡ 16 ä½å…ƒåœ–åƒã€‚

- **åŸºæ–¼è¦–è¦ºè¬é¡Œçš„æŠ€èƒ½å­¸ç¿’ç­–ç•¥**ï¼šæœ¬æ–‡å¼•å…¥äº†ä¸€ç¨®æ–°ç©çš„è¨“ç·´æ–¹æ³•ï¼Œé€éä¸‰å€‹ç‰¹åˆ¥è¨­è¨ˆçš„è¦–è¦ºè¬é¡Œï¼ˆè€Œéé…å°æ•¸æ“šçš„ç›´æ¥ç›£ç£å­¸ç¿’ï¼‰æ•™å° MLLM åœ–åƒæ“ä½œæ„ŸçŸ¥ã€‚è¬é¡Œ A æ•™å°å€‹åˆ¥æ“ä½œæ•ˆæœï¼ˆçµ¦å®šå‰å¾Œåœ–åƒï¼Œé æ¸¬æ“ä½œå’Œå€¼ï¼‰ï¼Œè¬é¡Œ B æ•™å°ç¾å­¸åˆ¤æ–·å’Œæœ€å„ªåƒæ•¸ç¯„åœï¼Œè¬é¡Œ C æ•™å°ç¶œåˆç·¨è¼¯è¨ˆåŠƒç”Ÿæˆã€‚æ­¤è¨­è¨ˆè§£æ±ºäº† MLLM å°åœ–åƒæ“ä½œæœ¬è³ªç¼ºä¹ç†è§£çš„æ ¹æœ¬å•é¡Œï¼Œä½¿å…¶èƒ½å¾æœ‰é™çš„å°ˆå®¶æ•¸æ“šï¼ˆè¬é¡Œ A ç´„ 7k æ¨£æœ¬ã€è¬é¡Œ B ç´„ 5kã€è¬é¡Œ C ç´„ 13kï¼‰å­¸ç¿’ã€‚

- **æ¨ç†å‹åŸºç¤æ•¸æ“šé›†åˆæˆ**ï¼šç‚ºå……åˆ†åˆ©ç”¨è¬é¡Œå‹å­¸ç¿’ï¼Œä½œè€…é€éå°‡é è¨“ç·´ LLMï¼ˆGemini 2.0 Flashï¼‰çš„æ¨ç†åŸºæ–¼å¯¦éš›è¦–è¦ºèª¿æ•´è€Œéå…è¨±å¹»è¦ºä¾†åˆæˆæ¨ç†è§£é‡‹ã€‚å°æ–¼æ¯å€‹è¬é¡Œå¯¦ä¾‹ï¼Œæ¨¡å‹ç”Ÿæˆçµæ§‹åŒ–æ¨ç†ï¼Œå½¢å¼ç‚º <Adjustment, Issue, Solution> ä¸‰å…ƒçµ„ï¼Œè§£é‡‹æ¯å€‹æ“ä½œå¦‚ä½•è§£æ±ºç‰¹å®šè¦–è¦ºå•é¡Œã€‚æ­¤æ¨ç†æ—¢ç”¨ä½œè¨“ç·´ä¿¡è™Ÿï¼Œä¹Ÿç”¨ä½œæ¨ç†è·¯å¾‘â€”åœ¨æ¨ç†æœŸé–“ï¼ŒMLLM é¦–å…ˆç”Ÿæˆé—œæ–¼åœ–åƒå•é¡Œçš„é«˜ç´šæ¨ç†ï¼Œç„¶å¾Œä½¿ç”¨æ­¤æ¨ç†å›æ­¸ç²¾ç¢ºåƒæ•¸å€¼ï¼Œæœ‰æ•ˆåœ°ä½¿ç”¨æ¨ç†ä½œç‚ºæ„åœ–åˆ°æ•¸å€¼èª¿æ•´çš„æ©‹æ¨‘ã€‚

- **å…·æœ‰ä½¿ç”¨è€…äº’å‹•æ€§çš„åˆ†æ®µç¨‹åºåŒ–æµç¨‹**ï¼šMonetGPT åˆ†ä¸‰å€‹éšæ®µæ‡‰ç”¨ç·¨è¼¯ï¼ˆå…‰ç·š â†’ è‰²å½©èª¿æ•´ â†’ ç‰¹å®šè‰²å½©å¾®èª¿ï¼‰ï¼Œå¯¦ç¾å¤šé …å„ªå‹¢ï¼š(1) é¡åˆ¥å…§æ“ä½œçš„å¯é€†æ€§ä»¥å¯é åœ°åˆæˆè¨“ç·´æ•¸æ“šï¼Œ(2) èˆ‡åŒæ™‚é€²è¡Œå¤šæ“ä½œè¦åŠƒç›¸æ¯”è¤‡é›œåº¦é™ä½ï¼Œ(3) é—œæ–¼å“ªå€‹æ“ä½œé€ æˆå“ªç¨®è¦–è¦ºæ•ˆæœçš„æ¨ç†æ›´æ¸…æ™°ã€‚è‡³é—œé‡è¦çš„æ˜¯ï¼ŒMLLM çš„è‡ªå›æ­¸æ€§è³ªä½¿ä½¿ç”¨è€…èƒ½åœ¨ä»»ä½•éšæ®µç·¨è¼¯å’Œè¦†è“‹è¨ˆåŠƒâ€”ä½¿ç”¨è€…å¯ä¿®æ”¹ä»»ä½•å»ºè­°çš„èª¿æ•´ï¼Œç³»çµ±æœƒè‡ªå‹•é‡æ–°è¦åŠƒå¾ŒçºŒéšæ®µæ“ä½œã€‚

- **èˆ‡ç”Ÿæˆå¼å’Œç¨‹åºåŒ–åŸºç·šçš„ç¶œåˆè©•ä¼°**ï¼šæœ¬æ–‡æä¾›å»£æ³›çš„è©•ä¼°ï¼Œæ¯”è¼ƒåŒ…æ‹¬ï¼šåŸºæ–¼ RL çš„æ–¹æ³•ï¼ˆExposureï¼‰ã€ç”Ÿæˆå¼æ–¹æ³•ï¼ˆMGIEã€InstructPix2Pixï¼‰ã€æœªé…å°å¢å¼·æ–¹æ³•ã€ç›´æ¥ MLLM è¿´æ­¸è®Šé«”ã€Gemini 2.0 èˆ‡æ€ç¶­éˆã€å•†æ¥­è»Ÿé«”ï¼ˆGoogle Photosï¼‰ã€‚è©•ä¼°åŒ…æ‹¬å®šé‡æŒ‡æ¨™ï¼ˆAdobe5k ä¸Š 400 å¼µåœ–åƒçš„ SSIMã€LPIPSã€PSNRã€ç›´æ–¹åœ–äº¤é›†ï¼‰ã€åŒ…å«æ–°æ‰‹ï¼ˆn=15ï¼‰å’Œå°ˆå®¶ï¼ˆn=10ï¼‰è©•å¯©è€…çš„å®šæ€§ä½¿ç”¨è€…ç ”ç©¶ï¼Œä»¥åŠå±•ç¤ºè¬é¡Œå‹è¨“ç·´ç›¸å°ç›´æ¥è¿´æ­¸ä¹‹å¿…è¦æ€§çš„æ¶ˆèç ”ç©¶ã€‚

- **å…·æœ‰ç°¡åŒ–åƒæ•¸ç©ºé–“çš„å¯¦ç”¨åº«å¯¦ç¾**ï¼šèˆ‡ä½¿ç”¨è¤‡é›œ GIMP ç­‰å·¥å…·ä¸åŒï¼Œä½œè€…é–‹ç™¼äº†åŒ…å« 33 å€‹æ¨¡çµ„åŒ–èª¿æ•´æ“ä½œçš„ Python åº«ï¼Œå…¶ä¸­æ¯å€‹æ“ä½œç”±å–®å€‹ä¸»åƒæ•¸æ§åˆ¶ï¼ˆå­åƒæ•¸è¦éº¼å›ºå®šè¦éº¼è¡ç”Ÿï¼‰ã€‚æ“ä½œéµå¾ªå¯æ„ŸçŸ¥ç·šæ€§ [-100, +100] æ¨™åº¦ï¼Œä½¿å¾®èª¿ MLLM èƒ½ç”Ÿæˆç›´æ¥åŸ·è¡Œçš„ JSON æ ¼å¼ç·¨è¼¯è¨ˆåŠƒï¼Œç„¡éœ€é¡å¤–ç·¨ç¢¼ï¼Œä½¿æ–¹æ³•å°æœ€çµ‚ä½¿ç”¨è€…å¯¦ç”¨ï¼ŒåŒæ™‚ä¿æŒå¯è§£é‡‹æ€§ã€‚

## æ ¸å¿ƒæ´è¦‹

- **æœªç¶“æ˜ç¢ºè¨“ç·´çš„ MLLM ç¼ºä¹æ“ä½œç†è§£**ï¼šæœ¬æ–‡è­‰æ˜äº†ä¸€å€‹é—œéµæ´è¦‹â€”å„˜ç®¡é è¨“ç·´ MLLM å…·æœ‰å¼·å¤§çš„è¦–è¦ºç†è§£èƒ½åŠ›ï¼Œä½†å®ƒå€‘å°åœ–åƒæ“ä½œçš„ä½œç”¨åŠå…¶å¦‚ä½•å½±éŸ¿åœ–åƒçš„å…§éƒ¨æ¨¡å‹åŸºæœ¬ä¸Šå¾ˆå·®ã€‚ç›´æ¥åœ¨é…å°ä¿®é£¾æ•¸æ“šä¸Šå¾®èª¿ï¼ˆã€ŒMLLM è¿´æ­¸ã€åŸºç·šï¼‰æœƒå¤§å¹…å¤±æ•—ï¼Œå› ç‚ºæ¨¡å‹ç„¡æ³•å°‡è¦–è¦ºè®ŠåŒ–èˆ‡ç‰¹å®šæ“ä½œè¯ç¹«èµ·ä¾†ã€‚é€™è§£é‡‹äº†ç‚ºä½•ç”¨æ“ä½œåº«æç¤º Gemini 2.0 ç­‰ç°¡å–®æ–¹æ³•è¡¨ç¾ä¸ä½³çš„åŸå› ã€‚è§£æ±ºæ–¹æ¡ˆâ€”é€éè¦–è¦ºè¬é¡Œçš„è¨“ç·´ï¼Œæ˜ç¢ºæ•™å°æ“ä½œèªç¾©â€”æ­ç¤ºæ“ä½œæ„ŸçŸ¥æ˜¯ä¸€é …å¯å­¸ç¿’çš„æŠ€èƒ½ï¼Œå¯è½‰ç§»åˆ°æº–ç¢ºçš„è¦åŠƒã€‚

- **æ¨ç†ä½œç‚ºåƒæ•¸è¿´æ­¸çš„è·¯å¾‘**ï¼šMonetGPT ä¸æ˜¯å¾åœ–åƒç›´æ¥è¿´æ­¸æ•¸å€¼åƒæ•¸ï¼ˆé€™æœƒåœ¨æœ‰é™æ•¸æ“šä¸‹å¼•èµ·éåº¦æ“¬åˆï¼‰ï¼Œè€Œæ˜¯ä½¿ç”¨æ€ç¶­éˆæ¨ç†ä½œç‚ºä¸­é–“è¡¨ç¤ºï¼Œæ©‹æ¥é«˜ç´šåœ–åƒåˆ†æåˆ°ç²¾ç¢ºåƒæ•¸å€¼ã€‚åœ¨æ¨ç†æœŸé–“ï¼Œæ¨¡å‹é¦–å…ˆç”Ÿæˆå•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆçš„èªç¾©æè¿°ï¼Œç„¶å¾Œå°‡å…¶è½‰è­¯ç‚ºæ•¸å€¼èª¿æ•´ã€‚æ­¤å…©éšæ®µéç¨‹ï¼ˆæ¨ç† â†’ è¿´æ­¸ï¼‰é¡¯è‘—å„ªæ–¼ç›´æ¥è¿´æ­¸åŸºç·šï¼Œè¡¨æ˜ LLM æ›´é©åˆèªç¾©æ¨ç†é—œæ–¼ç·¨è¼¯æ„åœ–ï¼Œè€Œéå¾è¦–è¦ºç‰¹å¾µçš„æ•¸å€¼è¿´æ­¸ã€‚

- **å¯é€†æ€§ä½¿å¤§è¦æ¨¡åˆæˆæ•¸æ“šç”Ÿæˆæˆç‚ºå¯èƒ½**ï¼šé€éé™åˆ¶åƒæ•¸é¡åˆ¥å…§çš„ä¿®æ”¹ä¿æŒå¯é€†ï¼ˆä¾‹å¦‚ï¼Œåƒ…ä¿®æ”¹å…‰ç·šæ“ä½œï¼Œç„¶å¾Œåƒ…ä¿®æ”¹è‰²å½©æ“ä½œï¼‰ï¼Œä½œè€…èƒ½ç¨‹åºåŒ–å¾å°ˆå®¶ç·¨è¼¯åœ–åƒç”Ÿæˆè¨“ç·´è¬é¡Œï¼Œç„¡éœ€é…å°çš„æºç›®æ¨™æ•¸æ“šã€‚ä»–å€‘å–æ¨£å°ˆå®¶ç·¨è¼¯åœ–åƒ ğ¼_Xï¼Œæ‡‰ç”¨éš¨æ©Ÿæ“ä½œæ“¾å‹•ä»¥å»ºç«‹é™ç´šç‰ˆæœ¬ ğ¼_Sï¼Œç„¶å¾Œä½¿ç”¨åŸºç¤æ¨ç†ç”Ÿæˆä»¥æ¨™è¨˜ç‚ºä½•æ¯å€‹æ“ä½œå¹«åŠ©æ¢å¾©å“è³ªã€‚æ­¤æœªé…å°æ–¹æ³•è‡³é—œé‡è¦â€”æ•¸æ“šé›†å®Œå…¨å¾å°ˆå®¶ç·¨è¼¯åˆæˆï¼Œè¨“ç·´æœŸé–“å¾æœªè¦‹éåŸå§‹åŸå§‹åœ–åƒï¼Œä½†æ³›åŒ–åˆ°æœªè¦‹éçš„æºåœ–åƒã€‚

- **åˆ†æ®µè¦åŠƒé™ä½ç‹€æ…‹ç©ºé–“ä¸¦æ”¹å–„æ¨ç†å“è³ª**ï¼šå°‡è¦åŠƒä»»å‹™åˆ†ç‚ºä¸‰å€‹éšæ®µï¼ˆå…‰ç·š â†’ è‰²å½©/æº«åº¦ â†’ ç‰¹å®šè‰²å½©ï¼‰ï¼Œè€Œéè¯åˆé æ¸¬æ‰€æœ‰æ“ä½œï¼Œå¤§å¹…ç°¡åŒ–å•é¡Œã€‚åœ– 7 å±•ç¤ºç›´æ¥è¿´æ­¸åœ¨ä¸åŒè¼¸å…¥æ¢ä»¶ä¸‹ç”¢ç”Ÿå¹¾ä¹ç›¸åŒçš„åƒæ•¸å€¼ï¼Œè€Œåˆ†æ®µ MonetGPT ç”¢ç”Ÿé©ç•¶è®ŠåŒ–çš„èª¿æ•´ã€‚æ­¤åˆ†æ®µä½¿é—œæ–¼å“ªäº›æ“ä½œè§£æ±ºå“ªäº›è¦–è¦ºå•é¡Œçš„å› æœæ¨ç†æ›´æ¸…æ™°â€”äººé¡ç„¡æ³•è¼•æ˜“è§£é‡‹ç‚ºä½•éœ€è¦ 15 å€‹åŒæ™‚æ“ä½œï¼Œä½†å¯æ¸…æ¥šé—¡è¿°åœ¨ç‰¹å®šå…‰ç·šæƒ…æ™¯ä¸‹ç‚ºä½•é»‘è‰²éœ€æå‡ã€é«˜å…‰éœ€é™ä½ã€‚

- **æ“ä½œæ„ŸçŸ¥è½‰ç§»åˆ°å¤šæ¨£åŒ–ç¾å­¸åå¥½**ï¼šåœ– 7ï¼ˆå³é¢æ¿ï¼‰é¡¯ç¤ºåŒä¸€æ“ä½œæ„ŸçŸ¥ MLLM å¯æ ¹æ“šé¢¨æ ¼æ¨™ç±¤ï¼ˆã€Œå¾©å¤ã€ã€ã€Œå¹³è¡¡ã€ã€ã€Œå……æ»¿æ´»åŠ›ã€ï¼‰åƒ…é€éä¸åŒçš„æç¤ºæ¢ä»¶ç”Ÿæˆä¸åŒçš„ç·¨è¼¯è¨ˆåŠƒã€‚æ­¤éˆæ´»æ€§å°æ–¼å›ºå®šåƒæ•¸è¿´æ­¸ä¸å¯èƒ½å¯¦ç¾ã€‚åŸºç¤æ“ä½œç†è§£å…è¨±æ¨¡å‹æ ¹æ“šé¢¨æ ¼æ„åœ–èª¿æ•´å…¶èª¿æ•´ï¼Œè¡¨æ˜æ•™å°æ©Ÿå™¨é—œæ–¼æ“ä½œèªç¾©çš„çŸ¥è­˜æ¯”æ•™å°ç‰¹å®šç·¨è¼¯æ¨¡å¼å¯è§£é–æ›´å¤§çš„é©æ‡‰æ€§ã€‚

- **èº«ä»½ä¿ç•™å’Œå¯è§£é‡‹æ€§èˆ‡ä½¿ç”¨è€…åå¥½ä¸€è‡´**ï¼šä½¿ç”¨è€…ç ”ç©¶é¡¯ç¤ºå° MonetGPT çš„å¼·çƒˆåå¥½ï¼ˆç›¸å° Exposure æˆ– MGIEï¼Œ60-70% çš„æ–°æ‰‹å’Œå°ˆå®¶ä½¿ç”¨è€…åå¥½ï¼‰ï¼Œæ­¤åå¥½èˆ‡èº«ä»½ä¿ç•™å’Œå¯è§£é‡‹æ€§ç›¸é—œã€‚å°ˆå®¶æ¬£è³èª¿æ•´çš„å¯è§£é‡‹æ€§ï¼Œå¯å»ºè­°å°çš„æ”¹é€²å¦‚ã€Œå¢åŠ è†šè‰²é£½å’Œåº¦ã€ï¼Œè¡¨æ˜ä»–å€‘ç†è§£ä¸”å¯è¦†è“‹ç³»çµ±ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¯èƒ½ä¿ç•™æˆ–æ”¹å–„æŒ‡æ¨™çš„ç”Ÿæˆå¼æ–¹æ³•ç”±æ–¼èº«ä»½ä¸Ÿå¤±ä»å¯èƒ½å¤±æ•—ä½¿ç”¨è€…åå¥½ï¼Œçªé¡¯ç¨‹åºåŒ–æ–¹æ³•å„˜ç®¡ PSNR å€¼ç•¥ä½ä½†æ›´å¥½åœ°æœå‹™å°ˆæ¥­å·¥ä½œæµç¨‹ã€‚

## é—œéµæ•¸æ“šèˆ‡çµæœ

| æ–¹æ³• | SSIM â†‘ | LPIPS â†“ | PSNR â†‘ | ç›´æ–¹åœ– â†‘ |
|--------|--------|--------|--------|------------|
| Exposure [Hu et al. 2018] | 0.63 | 0.14 | 15.12 | 47.21 |
| Unpaired [Kosugi et al. 2020] | 0.83 | 0.12 | 21.73 | 83.98 |
| RSFNet [Ouyang et al. 2023] | 0.88 | 0.08 | 21.85 | 80.26 |
| InstructPix2Pix [Brooks et al. 2023] | 0.61 | 0.22 | 16.99 | 73.90 |
| MGIE [Fu et al. 2024] | 0.74 | 0.08 | 22.94 | 79.95 |
| Gemini CoT + åº« | 0.80 | 0.14 | 17.83 | 63.71 |
| GooglePhotos | 0.90 | 0.06 | 25.86 | 86.47 |
| MLLM è¿´æ­¸ | 0.84 | 0.10 | 20.89 | 82.05 |
| **MonetGPTï¼ˆæœ¬æ–‡ï¼‰** | **0.90** | **0.07** | **23.75** | **79.50** |

**æ•¸æ“šé›†èˆ‡è©•ä¼°è¨­ç½®**ï¼šåœ¨ PPR10Kï¼ˆåƒ…å°ˆå®¶ Aï¼Œæœªé…å°è¨­ç½®å«åˆæˆè¬é¡Œï¼‰ä¸Šè¨“ç·´ï¼Œåœ¨ Adobe5k çš„ 400 å¼µåœ–åƒä¸Šæ¸¬è©¦ä»¥è©•ä¼°æ³›åŒ–ã€‚æŒ‡æ¨™åŒ…æ‹¬æ„ŸçŸ¥ç›¸ä¼¼æ€§ï¼ˆSSIMã€LPIPSï¼‰ã€åƒç´ ä¿çœŸåº¦ï¼ˆPSNRï¼‰å’Œå°æ¯”åº¦/å…‰åº¦/é£½å’Œåº¦çš„ç›´æ–¹åœ–äº¤é›†ã€‚ç”¨æˆ¶ç ”ç©¶ä½¿ç”¨ 50 å¼µ Adobe5k å’Œ Reddit åœ–åƒï¼Œå…± 15 å€‹æ–°æ‰‹å’Œ 10 ä½å°ˆå®¶è©•å¯©è€…ï¼ˆ200 å€‹äºŒé€²åˆ¶åå¥½å•é¡Œï¼‰ã€‚

- **å®šé‡æ€§èƒ½**ï¼šMonetGPT åœ¨å¤§å¤šæ•¸æŒ‡æ¨™ä¸Šé”åˆ°å…·ç«¶çˆ­åŠ›æˆ–é ˜å…ˆçš„æ€§èƒ½â€”SSIM èˆ‡ GooglePhotos ä¸¦åˆ—ï¼ˆ0.90ï¼‰ï¼Œå¯æ¯”è¼ƒçš„ LPIPSï¼ˆ0.07 å° 0.06ï¼‰ï¼Œåœ¨ PSNRï¼ˆ23.75ï¼‰ä¸Šè¶…éå¤§å¤šæ•¸é–‹æºåŸºç·šã€‚ç›´æ–¹åœ–æŒ‡æ¨™ï¼ˆ79.50ï¼‰è¡¨æ˜è‰¯å¥½çš„åˆ†ä½ˆåŒ¹é…ï¼Œå„˜ç®¡ç•¥ä½æ–¼ GooglePhotosï¼ˆ86.47ï¼‰å’Œ Unpairedï¼ˆ83.98ï¼‰ã€‚åœ¨ LPIPS ä¸Šï¼ŒMonetGPTï¼ˆ0.07ï¼‰åŒ¹é…æœ€ä½³è¡¨ç¾è€…ï¼ˆRSFNet 0.08ã€MGIE 0.08ï¼‰ï¼ŒåŒæ™‚ä½¿ç”¨å¯è§£é‡‹çš„ç¨‹åºåŒ–æ–¹æ³•è€Œéç«¯åˆ°ç«¯ç”Ÿæˆã€‚

- **ä½¿ç”¨è€…åå¥½åœ¨å„å°ˆæ¥­æ°´æº–é–“ä¸€è‡´**ï¼šåœ¨é¢å°é¢æ¯”è¼ƒä¸­ï¼ŒMonetGPT å° Exposure å’Œ MGIE åŸºç·šçš„åå¥½ç‡åˆ†åˆ¥ç´„ 65-70% çš„æ–°æ‰‹å’Œå°ˆå®¶ä½¿ç”¨è€…ã€‚åœ– 8 æ˜ç¢ºå±•ç¤ºè·¨ä½¿ç”¨è€…ç¾¤é«”çš„ä¸€è‡´åå¥½ï¼Œå°ˆå®¶æä¾›å»ºè¨­æ€§åé¥‹ï¼ˆä¾‹å¦‚ã€Œå¢åŠ è†šè‰²é£½å’Œåº¦ã€ã€ã€Œæ›´å¥½çš„æ›²ç·šå¼·èª¿ã€ï¼‰ï¼Œè¡¨æ˜ä»–å€‘ç†è§£ä¸¦ä¿¡ä»»æè­°çš„èª¿æ•´ã€‚æ­¤åå¥½å·®è·å€¼å¾—æ³¨æ„ï¼Œå› ç‚ºå®ƒèˆ‡ç´”ç²¹åŸºæ–¼æŒ‡æ¨™çš„æ’åç›¸çŸ›ç›¾â€”å…·æœ‰æ›´é«˜ PSNRï¼ˆGooglePhotos 25.86 å° MonetGPT 23.75ï¼‰çš„æ–¹æ³•åœ¨ä½¿ç”¨è€…ç ”ç©¶ä¸­ä¸¦éä¸€è‡´åå¥½ã€‚

- **æ¶ˆèï¼šç›´æ¥è¿´æ­¸æ€§èƒ½åš´é‡ä¸è¶³**ï¼šMLLM è¿´æ­¸åŸºç·šï¼ˆç›¸åŒæ¨¡å‹ã€æ¶æ§‹ã€è¨“ç·´æ•¸æ“šä½†è¨“ç·´ç‚ºç›´æ¥é æ¸¬åƒæ•¸ï¼‰åœ¨ SSIM 0.84ã€LPIPS 0.10ã€PSNR 20.89 ä¸Šé”æˆâ€”åœ¨ SSIMï¼ˆ0.90ï¼‰å’Œ PSNR ä¸Šé ä½æ–¼ MonetGPTã€‚åœ– 10 é¡¯ç¤ºè¿´æ­¸åŸºç·šé­å—åš´é‡éåº¦æ“¬åˆï¼šå®ƒåœ¨ 100 å¼µä¸åŒåœ–åƒä¸Šé æ¸¬å¹¾ä¹ç›¸åŒçš„åƒæ•¸å€¼ï¼Œç„¡æ³•æ•æ‰å®Œæ•´çš„èª¿æ•´ç¯„åœã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMonetGPT çš„æç´åœ–å±•ç¤ºå®ƒé©ç•¶åœ°ä½¿ç”¨å®Œæ•´çš„åƒæ•¸ç¯„åœ [-100, +100]ï¼Œç¢ºèªåŸºæ–¼æ¨ç†çš„è¦åŠƒå¯¦ç¾æ›´å¤§çš„å¤šæ¨£æ€§å’Œé©æ‡‰æ€§ã€‚

- **è¨ˆç®—æ•ˆç‡æ¬Šè¡¡**ï¼šå®Œæ•´åˆ†æ®µç®¡é“æ¨ç†åœ¨ RTX 4090 ä¸Šéœ€ç´„ 25 ç§’ï¼Œç›¸æ¯”ç´„ 10 ç§’ç”¨æ–¼ç›´æ¥è¿´æ­¸å’Œç´„ 2 ç§’ç”¨æ–¼ Exposureã€‚å„˜ç®¡æ¯”åŸºæ–¼ RL çš„æ–¹æ³•æ…¢ï¼Œæ­¤å°å¾Œè™•ç†å·¥ä½œæµç¨‹å¯æ¥å—ã€‚è¨“ç·´åœ¨ H100 GPU ä¸Šéœ€ç´„ 8 å°æ™‚ç”¨æ–¼è¬é¡Œå‹å¾®èª¿å°æ¯” 2.5 å°æ™‚ç”¨æ–¼è¿´æ­¸ï¼Œä»£è¡¨ 3.2 å€å¢åŠ ã€‚æ­¤æˆæœ¬ç”±æ–¼ç›¸å°æ›¿ä»£æ–¹æ¡ˆåœ¨æ³›åŒ–å’Œèº«ä»½ä¿ç•™ä¸Šçš„é¡¯è‘—æ”¹é€²è€Œæ­£ç•¶ã€‚

- **å°ä¸åŒé ˜åŸŸåœ–åƒçš„æ³›åŒ–**ï¼šåƒ…åœ¨ PPR10Kï¼ˆä¸»è¦ç‚ºè‚–åƒï¼‰ä¸Šè¨“ç·´ä½†åœ¨å¤šæ¨£åŒ– Adobe5k åœ–åƒä¸Šè©•ä¼°é¡¯ç¤ºåˆç†æ³›åŒ–ã€‚ç›´æ–¹åœ–äº¤é›†æŒ‡æ¨™ï¼ˆ79.50ï¼‰è¡¨æ˜é æ¸¬åˆ†ä½ˆå„˜ç®¡é ˜åŸŸè½‰ç§»ä»å¤§è‡´èˆ‡å°ˆå®¶ç›®æ¨™åŒ¹é…ã€‚ç„¶è€Œï¼Œç›¸åŒè©•ä¼°è¨­ç½®ä¸Šèˆ‡ GooglePhotos çš„æ¯”è¼ƒè¡¨æ˜æ”¹é€²ç©ºé–“â€”GooglePhotos å¯èƒ½åˆ©ç”¨ MonetGPT æ˜ç¢ºé¿å…ä¾è³´çš„å¤§å‹å°ˆæœ‰è¨“ç·´æ•¸æ“šé›†ã€‚

## å„ªå‹¢

- **æ–°ç©ä¸”è‰¯å¥½å‹•æ©Ÿçš„å•é¡Œè¡¨è¿°**ï¼šæœ¬æ–‡è§£æ±ºäº†ç”Ÿæˆå¼ç·¨è¼¯ï¼ˆä¸Ÿå¤±èº«ä»½ï¼‰å’Œå‚³çµ±ç¨‹åºåŒ–ç·¨è¼¯ï¼ˆé›£ä»¥è¦åŠƒï¼‰ä¹‹é–“çš„çœŸå¯¦å·®è·ã€‚å°‡ç¨‹åºåŒ–åœ–åƒä¿®é£¾ä½œç‚º MLLM æ¨ç†ä»»å‹™çš„æ¡†æ¶è¡¨è¿°æ˜¯æ–°ç©çš„ï¼ŒMLLM ç¼ºä¹æ“ä½œç†è§£çš„è§€å¯Ÿæ˜¯æœ‰æ´è¦‹çš„ä¸”æœ‰å……åˆ†è­‰æ“šæ”¯æŒã€‚è©²æ–¹æ³•æ¤æ ¹æ–¼äººé¡å­¸ç¿’ç†è«–ï¼ˆé€éè¬é¡Œè§£é¡Œçš„æŠ€èƒ½å­¸ç¿’ï¼‰ï¼Œç‚ºè¨­è¨ˆé¸æ“‡å¢åŠ äº†å¯ä¿¡åº¦ã€‚è©²å•é¡Œå°å°ˆæ¥­å·¥ä½œæµç¨‹å…·æœ‰æ˜ç¢ºçš„å¯¦è¸åƒ¹å€¼ï¼Œå…¶ä¸­å¯ç·¨è¼¯æ€§å’Œå¯è§£é‡‹æ€§æ˜¯ä¸å¯å•†é‡çš„ã€‚

- **é€éè¦–è¦ºè¬é¡Œçš„å‰µæ–°è¨“ç·´ç­–ç•¥**ï¼šä¸‰è¬é¡Œæ¡†æ¶æ˜¯å‰µæ„ä¸”ç²¾å¿ƒè¨­è¨ˆçš„ã€‚èˆ‡å…¶å¼·åˆ¶æ¨¡å‹å¾æœ‰é™é…å°æ•¸æ“šå­¸ç¿’ï¼Œä½œè€…å·§å¦™åœ°å°‡å­¸ç¿’åˆ†è§£ç‚ºèªç¾©å–®ä½ï¼šç†è§£å€‹åˆ¥æ“ä½œï¼ˆè¬é¡Œ Aï¼‰ã€ç¾å­¸åˆ¤æ–·ï¼ˆè¬é¡Œ Bï¼‰å’Œè¦åŠƒï¼ˆè¬é¡Œ Cï¼‰ã€‚é€²ç¨‹åœ¨æ•™å­¸ä¸Šåˆç†ï¼Œä½¿ç”¨åŸºç¤æ¨ç†ç”Ÿæˆä»¥é˜²æ­¢å¹»è¦ºåœ¨æŠ€è¡“ä¸Šè¤‡é›œã€‚æ­¤æ–¹æ³•å¯æ¨å»£åˆ°å…¶ä»–éœ€è¦æ“ä½œç†è§£çš„é ˜åŸŸï¼Œä¸¦å¯èƒ½å½±éŸ¿æœªä¾†å¤šæ¨¡æ…‹æ¨¡å‹æŠ€èƒ½å­¸ç¿’çš„å·¥ä½œã€‚

- **ç¶œåˆæ€§å¯¦é©—è©•ä¼°**ï¼šæœ¬æ–‡æ¯”è¼ƒè·¨è¶Šä¸åŒåŸºç·šçš„å¤šæ¨£æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºæ–¼ RLï¼ˆExposureï¼‰ã€ç¥ç¶“è¿´æ­¸ï¼ˆRSFNetï¼‰ã€ç”Ÿæˆå¼ï¼ˆMGIEã€InstructPix2Pixï¼‰å’Œå•†æ¥­è§£æ±ºæ–¹æ¡ˆï¼ˆGooglePhotosï¼‰ã€‚è©•ä¼°åŒ…æ‹¬å¤šå€‹ç¶­åº¦ï¼šå®šé‡æŒ‡æ¨™ï¼ˆSSIMã€LPIPSã€PSNRã€ç›´æ–¹åœ–ï¼‰ã€åŒ…å«å°ˆå®¶å’Œæ–°æ‰‹çš„ä½¿ç”¨è€…ç ”ç©¶ã€å±•ç¤ºè¬é¡Œå‹è¨“ç·´å¿…è¦æ€§çš„æ¶ˆèï¼Œä»¥åŠå«å°ˆå®¶åé¥‹çš„å®šæ€§åˆ†æã€‚è£œå……ææ–™ä¼¼ä¹å»£æ³›ï¼ˆå¤šæ¬¡åƒè€ƒï¼‰ã€‚å…± 25 ä½è©•å¯©è€…çš„ä½¿ç”¨è€…ç ”ç©¶å°æ­¤é ˜åŸŸè€Œè¨€è¦æ¨¡åˆç†ã€‚

- **å¯¦ç”¨å¯¦ç¾å’Œå¯é‡ç¾æ€§**ï¼šä½œè€…æ‰¿è«¾ç™¼ä½ˆä»£ç¢¼ã€æ•¸æ“šã€æ¨¡å‹å’Œè£œå……çµæœã€‚æœ¬æ–‡æä¾›å……è¶³çš„å¯¦ç¾ç´°ç¯€ï¼šQwen-VL-7B ç”¨ DoRA é©é…å™¨å¾®èª¿ã€ç‰¹å®šè¶…åƒæ•¸ï¼ˆå­¸ç¿’ç‡ 1e-4ã€ç§© 256ã€å–®è¼ªæ¬¡ï¼‰ã€æ•¸æ“šé›†å¤§å°ï¼ˆ7kã€5kã€13k æ¨£æœ¬ï¼‰å’Œåº«è¦ç¯„ï¼ˆ33 æ“ä½œï¼‰ã€‚é¸æ“‡é–‹ç™¼ç°¡åŒ– Python åº«è€Œéä¾è³´ GIMP é¡¯ç¤ºå°å¯¦è¸éƒ¨ç½²çš„é—œæ³¨ã€‚åŸºæ–¼ JSON çš„è¼¸å‡ºæ ¼å¼ä½¿æ•´åˆç›´æ¥ã€‚

- **å¯è§£é‡‹æ€§å’Œä½¿ç”¨è€…æ§åˆ¶**ï¼šèˆ‡é»‘ç›’ç”Ÿæˆå¼æ–¹æ³•ä¸åŒï¼ŒMonetGPT ç‚ºæ¯å€‹èª¿æ•´æä¾›äººé¡å¯è®€çš„æ¨ç†ä¸¦å•Ÿç”¨åˆ†æ®µç·¨è¼¯ã€‚åœ– 9 å±•ç¤ºè‡ªå›æ­¸ç·¨è¼¯ï¼Œå…¶ä¸­ä½¿ç”¨è€…å¯åœ¨ä»»ä½•éšæ®µä¿®æ”¹æè­°çš„è¨ˆåŠƒï¼Œä¸”ç³»çµ±é‡æ–°è¦åŠƒå¾ŒçºŒæ“ä½œâ€”é€™æ˜¯é‡è¦çš„å¯ç”¨æ€§å„ªå‹¢ã€‚<Adjustment, Issue, Solution> çµæ§‹åŒ–æ¨ç†æ ¼å¼å¯è§£é‡‹ä¸”å¯è¡Œå‹•ï¼Œä½¿é ˜åŸŸå°ˆå®¶èƒ½é©—è­‰å’Œç´°åŒ–å»ºè­°ã€‚

- **ç¶“éä»”ç´°è€ƒæ…®çš„è¨­è¨ˆé¸æ“‡åŠæ˜ç¢ºåˆç†åŒ–**ï¼šåˆ†æ®µç®¡é“è¨­è¨ˆï¼ˆç¬¬ 4.3 ç¯€ï¼‰ç”±ä¸‰é …è€ƒæ…®åˆç†åŒ–ï¼šå¯é€†æ€§ç”¨æ–¼æ•¸æ“šåˆæˆã€è¤‡é›œåº¦é™ä½ã€æ›´æ¸…æ™°çš„æ¨ç†ã€‚Qwen-VL å°æ½›åœ¨æ›´å¤§æ¨¡å‹çš„é¸æ“‡ã€åƒæ•¸é‡åŒ–è‡³ [-100, +100] ä»¥ä¾¿æ–¼æ¨™è¨˜åŒ–ï¼Œä»¥åŠæ¨ç†æ•¸æ“šé›†çš„çµæ§‹éƒ½é¡¯ç¤ºæ…é‡è€ƒæ…®ã€‚æœ¬æ–‡è§£é‡‹ä¸åƒ… WHAT ä»–å€‘åšäº†ä»€éº¼ï¼Œä¹Ÿè§£é‡‹ WHYï¼Œé€™æœ‰åŠ©æ–¼å¯é‡ç¾æ€§å’Œæœªä¾†å·¥ä½œã€‚

## åŠ£å‹¢

- **æœ‰é™çš„è¨“ç·´æ•¸æ“šå¤šæ¨£æ€§å’Œæ½›åœ¨é ˜åŸŸè½‰ç§»**ï¼šæ¨¡å‹åƒ…åœ¨ PPR10K å°ˆå®¶ A ä¸Šç”¨ç´„ 25k åˆæˆè¬é¡Œæ¨£æœ¬è¨“ç·´ï¼Œç„¶å¾Œåœ¨ Adobe5k ä¸Šè©•ä¼°ã€‚PPR10K ä¸»è¦ç‚ºè‚–åƒåœ–åƒï¼Œè€Œ Adobe5k åŒ…å«æ›´å¤šæ¨£åŒ–çš„å ´æ™¯ã€‚å„˜ç®¡æœ¬æ–‡è²ç¨±æ³›åŒ–ï¼Œå®šé‡çµæœæ­ç¤ºå·®è·ï¼šç›´æ–¹åœ–äº¤é›†ï¼ˆ79.50ï¼‰æ˜é¡¯ä½æ–¼ GooglePhotosï¼ˆ86.47ï¼‰å’Œå¤šå€‹åŸºç·šã€‚æœ¬æ–‡æœªæä¾›è·¨é ˜åŸŸæ¶ˆèï¼ˆä¾‹å¦‚ï¼Œåœ¨å…©å€‹ PPR10K å°ˆå®¶ä¸Šè¨“ç·´ã€åœ¨ PPR10K ä¿ç•™ä¸Šè©•ä¼°ï¼‰ä»¥éš”é›¢é ˜åŸŸè½‰ç§»çš„å½±éŸ¿å°æ¯”æ–¹æ³•æœ¬èº«ã€‚GooglePhotos çš„å¼·å¤§æ€§èƒ½å»ºè­°æ›´å¤§ã€æ›´å¤šæ¨£çš„è¨“ç·´æ•¸æ“šå¯èƒ½è‡³é—œé‡è¦ï¼Œä½†æ­¤é™åˆ¶æœªè¢«æ·±å…¥åˆ†æã€‚

- **è¬é¡Œç”Ÿæˆä¸æ¸…æ¥šåŠæ½›åœ¨æ•¸æ“šæ´©éœ²**ï¼šæœ¬æ–‡æŒ‡å‡ºè¬é¡Œè¢«ã€Œåˆæˆåœ°ã€ç”Ÿæˆï¼Œé€éå°å°ˆå®¶åœ–åƒæ‡‰ç”¨éš¨æ©Ÿæ“ä½œï¼Œä½†é—œéµç´°ç¯€ç¼ºå¤±ã€‚æ“ä½œåƒæ•¸å¦‚ä½•å–æ¨£ï¼Ÿåˆ†ä½ˆæ˜¯ä»€éº¼ï¼Ÿå°æ–¼è¬é¡Œ Bï¼Œæœ¬æ–‡å‡è¨­ã€Œå° ğ¼_X é€²è¡Œä»»ä½•å……åˆ†å¤§çš„èª¿æ•´æœƒé™ä½åœ–åƒå“è³ªã€â€”æ­¤ç‚ºæœªé©—è­‰çš„å¼·å‡è¨­ã€‚è¬é¡Œ B ä¸­å››å€‹æ“¾å‹•ç´šåˆ¥å¦‚ä½•é¸æ“‡ï¼Ÿæ˜¯å¦æœ‰åŸå‰‡åˆ†ä½ˆï¼Œæˆ–æ˜¯è‡¨æ™‚æ€§çš„ï¼Ÿæ¸¬è©¦åœ–åƒä¾†è‡ªä¸åŒæ•¸æ“šé›†ï¼ˆAdobe5k å° PPR10Kï¼‰ï¼Œä½†é€éå¾å°ˆå®¶åœ–åƒç”Ÿæˆçš„åˆæˆè¬é¡Œçš„æ½›åœ¨æ•¸æ“šæ´©éœ²æœªè¢«è¨è«–ã€‚è‹¥ Adobe5k ç”¨æ–¼ç”Ÿæˆè¬é¡Œï¼Œçµæœæœƒè™›é«˜ã€‚

- **å¤±æ•—æ¡ˆä¾‹å’Œé™åˆ¶çš„åˆ†æä¸è¶³**ï¼šæœ¬æ–‡å±•ç¤ºä¸»è¦æˆåŠŸä¾‹å­ï¼Œä½†å°ä½•æ™‚åŠç‚ºä½•æ–¹æ³•å¤±æ•—çš„è¨è«–æœ‰é™ã€‚åœ– 6 å±•ç¤ºå®šæ€§æ¯”è¼ƒï¼Œä½†æœªçªé¡¯å¤±æ•—æ¨¡å¼ã€‚MonetGPT èˆ‡å“ªäº›é¡å‹åœ–åƒå°æŠ—ï¼Ÿæœ¬æ–‡æåŠå°ˆå®¶å»ºè­°ã€Œå°çš„æ”¹é€²ã€ï¼Œä½†æœªé‡åŒ–ä½•æ™‚éœ€è¦é€™äº›æˆ–å®ƒå€‘æœ‰å¤šé‡è¦ã€‚æ¨ç†æ™‚é–“ï¼ˆ25 ç§’ï¼‰å° Exposureï¼ˆ2 ç§’ï¼‰å¯¦è³ªä¸Šæ…¢â€”å°æ–¼å°ˆæ¥­å·¥ä½œæµç¨‹ï¼Œé€™å¯æ¥å—å—ï¼Ÿæœ¬æ–‡æœªè¨è«–æ­¤æ¬Šè¡¡ã€‚æ­¤å¤–ï¼Œæ–¹æ³•é™åˆ¶æ–¼åº«ä¸­ 33 å€‹æ“ä½œï¼›ç•¶å‘ˆç¾æœƒå—ç›Šæ–¼æ­¤é›†å¤–æ“ä½œçš„åœ–åƒæ™‚ï¼Œå®ƒå¦‚ä½•è¡¨ç¾ï¼Ÿ

- **èˆ‡ Gemini 2.0 çš„å¼±ç›´æ¥æ¯”è¼ƒåŠæœ‰é™æç¤ºæ¢ç´¢**ï¼šã€ŒGemini+Library CoTã€åŸºç·šå°æ–¼éš”é›¢è¬é¡Œå‹å¾®èª¿çš„ç›Šè™•è‡³é—œé‡è¦ï¼Œä½†ä¼¼ä¹æ¢ç´¢ä¸è¶³ã€‚æœ¬æ–‡æåŠä¸‰éšæ®µæç¤ºå«åº«åå’Œæ“ä½œé¡åˆ¥ï¼Œä½†æœªåœ¨ä¸»æ–‡æœ¬ä¸­æä¾›å®Œæ•´æç¤ºï¼ˆåƒ…åƒè€ƒè£œå……ï¼‰ã€‚å…·æœ‰æ­£ç¢ºçš„æç¤ºï¼ŒGemini 2.0 ç­‰è¤‡é›œæ¨¡å‹å¯èƒ½è¡¨ç¾å¯æ¯”ã€‚æœ¬æ–‡å±•ç¤ºå®šæ€§ä¾‹å­å…¶ä¸­ Gemini ç”¢ç”Ÿéåº¦æ›å…‰åœ–åƒï¼Œä½†ç„¡ç³»çµ±åˆ†æç‚ºä½•è¬é¡Œå‹æ–¹æ³•åŸºæœ¬ä¸Šå„ªæ–¼æ›´å¥½æç¤ºç­–ç•¥ã€‚æ­¤ç‚ºéš”é›¢åŸºç·šï¼Œå°æ–¼è­‰å¯¦å¾®èª¿åŠªåŠ›è‡³é—œé‡è¦ã€‚

- **ç¼ºå¤±æ¨ç†å“è³ªçš„å®šé‡åˆ†æ**ï¼šå„˜ç®¡æ¨ç†ç”Ÿæˆå°æ–¹æ³•è‡³é—œé‡è¦ï¼Œæœ¬æ–‡æœªæä¾›æ¨ç†å“è³ªçš„å®šé‡è©•ä¼°ã€‚<Adjustment, Issue, Solution> ä¸‰å…ƒçµ„æº–ç¢ºå—ï¼Ÿäººé¡å¦‚ä½•è©•å¯©è§£é‡‹ï¼Ÿæ¨ç†å“è³ªæ˜¯å¦èˆ‡ç·¨è¼¯å“è³ªç›¸é—œï¼Ÿæœ¬æ–‡å‡è¨­åŸºæ–¼å¯¦éš›èª¿æ•´çš„åŸºç¤é˜²æ­¢å¹»è¦ºï¼Œä½†æœªæä¾›è­‰æ“šã€‚æ¨ç†çš„å®šé‡è©•ä¼°â€”ä¾‹å¦‚é€éå°ˆå®¶äººé¡è©•ç´šæˆ–èˆ‡åœ°é¢å¯¦æ³èª¿æ•´çš„ä¿¡æ¯é‡ç–Šâ€”æœƒåŠ å¼·é—œæ–¼æ–¹æ³•å¯è§£é‡‹æ€§çš„è²ç¨±ã€‚åœ– 1 å±•ç¤ºè‰¯å¥½æ¨ç†çš„ä¾‹å­ï¼Œä½†æ­¤æ˜¯å¦ä»£è¡¨æ€§ï¼Ÿ

- **æœ‰é™çš„æ›¿ä»£è¨­è¨ˆé¸æ“‡æ¢ç´¢**ï¼šæœ¬æ–‡æ‰¿è«¾ç‰¹å®šé¸æ“‡ï¼Œæœªå¾¹åº•åˆç†åŒ–æ›¿ä»£æ–¹æ¡ˆã€‚ç‚ºä½•ä¸‰å€‹éšæ®µè€Œéå…©å€‹æˆ–å››å€‹ï¼Ÿç‚ºä½•è¬é¡Œ C çš„å‡è¨­ï¼Œæ“¾äº‚å°ˆå®¶ç·¨è¼¯ç”¢ç”Ÿè¼ƒå·®åœ–åƒï¼Ÿç‚ºä½•å¾®èª¿ Qwen-VL-7B è€Œéè¼ƒå¤§æ¨¡å‹å¦‚ Gemini 2.0 æˆ– GPT-4Vï¼Ÿæœ¬æ–‡æåŠä½¿ç”¨ Gemini 2.0 Flash ç”¨æ–¼æ¨ç†ç”Ÿæˆï¼Œä½†æœªå¾®èª¿ Gemini æœ¬èº«â€”ç‚ºä½•ï¼Ÿ8 å°æ™‚è¨“ç·´çš„è¨ˆç®—æˆæœ¬å»ºè­°å°å‹æ¨¡å‹åœ¨éƒ¨åˆ†ç¨‹åº¦ä¸Šå‡ºæ–¼æ•ˆç‡è€ƒæ…®è¢«é¸æ“‡ï¼Œä½†æ­¤æ¬Šè¡¡ä¸æ˜ç¢ºã€‚é€™äº›è¨­è¨ˆé¸æ“‡çš„æ¶ˆèæœƒé¡¯è‘—åŠ å¼·å·¥ä½œã€‚

- **è©•ä¼°æŒ‡æ¨™å¯èƒ½æœªå®Œå…¨æ•æ‰æ„ŸçŸ¥å“è³ª**ï¼šå„˜ç®¡ä½¿ç”¨è€…ç ”ç©¶æœ‰åƒ¹å€¼ï¼ŒæŸäº›é—œåˆ‡ä»å­˜åœ¨ã€‚PSNR/SSIM å·²ç´å…¥ä½†å·²çŸ¥èˆ‡äººé¡æ„ŸçŸ¥åœ–åƒå“è³ªç›¸é—œæ€§å·®â€”æœ¬æ–‡æ›´ä¾è³´ä½¿ç”¨è€…ç ”ç©¶ï¼Œé€™æ˜¯å¥½çš„ä½†é™åˆ¶äº†å¯é‡ç¾æ€§å’Œèˆ‡æœªä¾†å·¥ä½œçš„æ¯”è¼ƒã€‚ç›´æ–¹åœ–æŒ‡æ¨™æ¸¬é‡åˆ†ä½ˆåŒ¹é…ä½†éæ„ŸçŸ¥çœŸå¯¦æ€§ã€‚æœªå ±å‘Šæ„ŸçŸ¥å“è³ªæŒ‡æ¨™ï¼ˆä¾‹å¦‚ NIQEã€BRISQUEï¼‰ã€‚ã€Œå°ä»»ä½•å°ˆå®¶çš„æœ€é«˜åˆ†ã€è©•ä¼°è¦å‰‡ï¼ˆå– 5 ä½ Adobe5k å°ˆå®¶çš„æœ€å¤§å€¼ï¼‰å¯èƒ½è™›é«˜çµæœâ€”è·¨å°ˆå®¶å¹³å‡å°éå°ˆé–€é‡å°ä»–å€‘é¢¨æ ¼è¨“ç·´çš„æ–¹æ³•å¯èƒ½æ›´å…¬å¹³ã€‚æœ¬æ–‡æ‡‰åˆç†åŒ–æ­¤é¸æ“‡ä¸¦å ±å‘Šå…©å€‹è®Šé«”ã€‚

## ç ”ç©¶æ–¹å‘

- **å¤šå°ˆå®¶è‡ªé©æ‡‰å¾®èª¿èˆ‡é¢¨æ ¼è½‰ç§»**ï¼šæ“´å±• MonetGPT ä»¥åœ¨å¤šå€‹å°ˆå®¶ç·¨è¼¯è€…ä¸ŠåŒæ™‚å¾®èª¿ï¼ˆä¾‹å¦‚ï¼ŒAdobe5k ä¸­æ‰€æœ‰ 5 ä½å°ˆå®¶ï¼‰ï¼Œè€Œéå–®å€‹å°ˆå®¶ï¼Œç„¶å¾Œå­¸ç¿’å…è¨±æ¨¡å‹æ ¹æ“šä½¿ç”¨è€…åå¥½å‹•æ…‹è½‰ç§»ç·¨è¼¯é¢¨æ ¼çš„é¢¨æ ¼é©é…å™¨ã€‚æ­¤å¯é€éä»¥ä¸‹é”æˆï¼š(1) åœ¨å«å°ˆå®¶æ¨™ç±¤çš„å¤šå°ˆå®¶è¬é¡Œä¸Šè¨“ç·´åŸºç¤æ¨¡å‹ï¼Œ(2) å¼•å…¥è¼•é‡ç´šé¢¨æ ¼ç‰¹å®š LoRA é©é…å™¨ï¼Œ(3) ä½¿ç”¨æˆ¶èƒ½æŒ‡å®šæ‰€éœ€é¢¨æ ¼ä½œç‚ºè¼¸å…¥ã€‚é€™å°‡è§£æ±ºç•¶å‰å–®å°ˆå®¶åè¦‹çš„é™åˆ¶ï¼Œä¸¦ä½¿æ¨¡å‹èƒ½æ•æ‰å°ˆæ¥­ç·¨è¼¯åå¥½çš„å¤šæ¨£æ€§ï¼Œå»ºç«‹æ›´å¯æ³›åŒ–å’Œå€‹æ€§åŒ–çš„ç³»çµ±ã€‚æ­¤æ–¹æ³•å°æ–¼å¯¦è¸éƒ¨ç½²é«˜åº¦ç›¸é—œï¼Œå…¶ä¸­ä¸åŒä½¿ç”¨è€…å…·æœ‰ä¸åŒçš„ç¾å­¸åå¥½ã€‚

- **æ“ä½œåƒæ•¸å’Œæ¡æ¨£ç­–ç•¥çš„è¯åˆå­¸ç¿’**ï¼šèª¿æŸ¥æ¨¡å‹æ˜¯å¦èƒ½å­¸ç¿’ä¸åƒ…åƒæ•¸å€¼ï¼Œä¹Ÿå­¸ç¿’æ‡‰ç”¨å“ªäº›æ“ä½œï¼ˆç¨€ç–æ“ä½œé¸æ“‡ï¼‰ã€‚ç•¶å‰æ–¹æ³•ç¸½æ˜¯åœ¨ JSON ä¸­è¼¸å‡ºæ‰€æœ‰åƒæ•¸ã€‚é–‹ç™¼ä¸€å€‹è®Šé«”ï¼Œå…¶ä¸­æ¨¡å‹å­¸ç¿’æ±ºå®šå“ªäº›æ“ä½œå¿…è¦ï¼Œå“ªäº›å¯ç•¥éï¼Œé€²ä¸€æ­¥é™ä½åƒæ•¸ç©ºé–“ã€‚é€™å¯æ¶‰åŠï¼š(1) æ·»åŠ å­¸ç¿’çš„é–˜æ©Ÿåˆ¶ä»¥ç¢ºå®šæ“ä½œå¿…è¦æ€§ï¼Œ(2) ç”¨ç¨€ç–èª˜å°æå¤±è¨“ç·´ï¼Œ(3) è©•ä¼°ä½¿ç”¨è€…æ˜¯å¦æ›´åå¥½ç¨€ç–è¨ˆåŠƒã€‚æ­¤ç ”ç©¶ä½¿æ¨¡å‹èƒ½è½‰ç§»åˆ°éœ€è¦åŸºæœ¬ä¸åŒç·¨è¼¯ç­–ç•¥çš„åœ–åƒï¼Œä¸¦æ¸›å°‘ä¸å¿…è¦çš„æ“ä½œã€‚

- **å…·æœ‰ä½¿ç”¨è€…åœ¨è¿´è·¯åé¥‹çš„åˆ†å±¤è¦åŠƒ**ï¼šå°‡è‡ªå›æ­¸ç·¨è¼¯èƒ½åŠ›ï¼ˆåœ– 9ï¼‰æ“´å±•åˆ°å®Œæ•´çš„äº’å‹•ç³»çµ±ï¼Œå…¶ä¸­ä½¿ç”¨è€…åœ¨æ¯å€‹éšæ®µæä¾›ä¸­é–“åé¥‹ï¼Œæ¨¡å‹åŸºæ–¼æ­¤åé¥‹ç´°åŒ–å¾ŒçºŒéšæ®µè¦åŠƒã€‚é–‹ç™¼å¼·åŒ–å­¸ç¿’æ¡†æ¶ï¼Œå…¶ä¸­ï¼š(1) ä½¿ç”¨è€…æ¥å—/æ‹’çµ•ä¸­é–“ç·¨è¼¯æä¾›çå‹µä¿¡è™Ÿï¼Œ(2) æ¨¡å‹é€éäº’å‹•å­¸ç¿’é æ¸¬ä½¿ç”¨è€…åå¥½ï¼Œ(3) ç³»çµ±æ ¹æ“šç´¯ç©åé¥‹æ”¹é€²ç·¨è¼¯è¨ˆåŠƒã€‚æ­¤å°‡éœ€è¦æ”¶é›†ä½¿ç”¨è€…äº’å‹•æ•¸æ“šé›†ï¼Œè¨­è¨ˆé©ç•¶çš„ RL ç›®æ¨™ã€‚çµæœå°‡æ˜¯ä¸€å€‹åŠ©æ‰‹å¼ç³»çµ±ï¼Œåœ¨éƒ¨ç½²æœŸé–“å¾ä½¿ç”¨è€…å­¸ç¿’ï¼Œèˆ‡å°ˆæ¥­å’Œæ¥­é¤˜å·¥ä½œæµç¨‹ç›¸é—œã€‚

- **è·¨é ˜åŸŸæ³›åŒ–å’Œè‡ªå‹•æ“ä½œç™¼ç¾**ï¼šç•¶å‰ MonetGPT é™æ–¼ 33 å€‹é å®šç¾©æ“ä½œã€‚èª¿æŸ¥æ“ä½œæ„ŸçŸ¥å­¸ç¿’æ©Ÿåˆ¶æ˜¯å¦æ³›åŒ–åˆ°æ–°é ˜åŸŸå’Œåœ–åƒé¡å‹ï¼ˆä¾‹å¦‚ï¼Œè¦–é »ã€é†«ç™‚åœ–åƒï¼‰ï¼Œé€éå­¸ç¿’å¾ä»»å‹™ç‰¹å®šåœ–åƒå°ç™¼ç¾é©ç•¶æ“ä½œã€‚æ–¹æ³•ï¼š(1) ç‚ºæ–°é ˜åŸŸæä¾›æ¨¡å‹å«å·®åˆ†æ“ä½œçš„åº«ï¼Œ(2) ç”¨é ˜åŸŸç‰¹å®šå°ˆå®¶ç·¨è¼¯ä½¿ç”¨è¬é¡Œå‹è¨“ç·´ç­–ç•¥ï¼Œ(3) è©•ä¼°æ³›åŒ–ã€‚æ­¤å°‡å±•ç¤ºè¬é¡Œå­¸ç¿’å…¸ç¯„çš„é€šç”¨æ€§ï¼Œä¸¦ä½¿æ‡‰ç”¨æ–¼å°ˆæ¥­é ˜åŸŸå¦‚é†«ç™‚åœ–åƒå¢å¼·ã€ç§‘å­¸å¯è¦–åŒ–æˆ–è¦–é »ä¿®é£¾æˆç‚ºå¯èƒ½ï¼Œå…¶ä¸­å°ˆå®¶æŒ‡å—èˆ‡æ¨™æº–æ”å½±å¤§å¹…ä¸åŒã€‚

- **æ“ä½œæ„ŸçŸ¥å’Œè½‰ç§»å­¸ç¿’çš„ç†è«–åˆ†æ**ï¼šé€éä»¥ä¸‹æ–¹å¼é€²è¡Œå°ç‚ºä½•è¬é¡Œå‹è¨“ç·´æœ‰æ•ˆçš„æ·±å…¥èª¿æŸ¥ï¼š(1) åˆ†ææ¨¡å‹å­¸ç¿’çš„è¡¨ç¤ºï¼ˆæ¢æ¸¬ç ”ç©¶ä»¥è­˜åˆ¥æ“ä½œç‰¹å®šç‰¹å¾µï¼‰ï¼Œ(2) ç ”ç©¶è¬é¡Œåˆ°è¦åŠƒçš„è½‰ç§»ï¼ˆè¬é¡Œ A ä¸­çš„æ”¹é€²æ˜¯å¦ç›´æ¥é æ¸¬ç·¨è¼¯å“è³ªæ”¹é€²ï¼Ÿï¼‰ï¼Œ(3) æ¸¬é‡å­¸ç¿’æ“ä½œè¡¨ç¤ºçš„èªç¾©ç›¸ä¼¼æ€§ã€‚æ­¤å¯ç”¢ç”Ÿé—œæ–¼å¤šæ¨¡æ…‹æ¨¡å‹å¦‚ä½•å­¸ç¿’æŠ½è±¡æ¦‚å¿µï¼ˆæ“ä½œï¼‰å°æ¯”æ„ŸçŸ¥ç‰¹å¾µï¼ˆåœ–åƒå¤–è§€ï¼‰çš„ç†è«–æ´è¦‹ï¼Œä¸¦é€šçŸ¥è¨­è¨ˆæ›´å¥½è¨“ç·´ç›®æ¨™ã€‚æ­¤åˆ†æå¯å½±éŸ¿æˆ‘å€‘å¦‚ä½•ç‚ºéœ€è¦ç³»çµ±ç†è§£æŠ½è±¡å·¥å…·/æ“ä½œçš„å…¶ä»–é ˜åŸŸè¨“ç·´ LLMã€‚

- **å¯è§£é‡‹åœ–åƒç·¨è¼¯çš„è©•ä¼°æ¡†æ¶**ï¼šç‚ºå¯è§£é‡‹åœ–åƒç·¨è¼¯é–‹ç™¼ç¶œåˆåŸºæº–å’Œè©•ä¼°å”è­°ã€‚ç•¶å‰å·¥ä½œä½¿ç”¨æ¨™æº–æŒ‡æ¨™ï¼ˆSSIMã€LPIPSï¼‰ï¼Œå€Ÿè‡ªç„¡æ¢ä»¶ç”Ÿæˆï¼Œå¯èƒ½æœªæ•æ‰å¯è§£é‡‹æ€§å“è³ªã€‚æè­°æ¸¬é‡ä»¥ä¸‹çš„æŒ‡æ¨™ï¼š(1) æ¨ç†æº–ç¢ºæ€§ï¼ˆè§£é‡‹å¦‚ä½•åŒ¹é…å¯¦éš›èª¿æ•´ï¼‰ï¼Œ(2) æ¨ç†å¯è¡Œæ€§ï¼ˆä½¿ç”¨è€…èƒ½å¦åŸºæ–¼è§£é‡‹ç´°åŒ–å»ºè­°ï¼‰ï¼Œ(3) è¨ˆåŠƒç›¸å¹²æ€§ï¼ˆæ“ä½œæ˜¯å¦è§£æ±ºäº’è£œå•é¡Œæˆ–å†—é¤˜é‡å°åŒä¸€æ–¹é¢ï¼‰ã€‚å¯¦æ–½å«é‡åŒ–è©•ç´šå°ºåº¦è€ŒéäºŒé€²åˆ¶åå¥½çš„ä½¿ç”¨è€…ç ”ç©¶å”è­°ã€‚æ­¤æ¡†æ¶å°‡å•Ÿç”¨å°æœªä¾†å¯è§£é‡‹ç·¨è¼¯æ–¹æ³•çš„å…¬å¹³è©•ä¼°ï¼Œå¯æˆç‚ºæ¨™æº–åŸºæº–ã€‚

- **å°æ¨£æœ¬é©æ‡‰å’Œå¿«é€Ÿé¢¨æ ¼è‡ªè¨‚**ï¼šèª¿æŸ¥æ“ä½œæ„ŸçŸ¥æ¨¡å‹æ˜¯å¦èƒ½ç”¨å¹¾å€‹ä¾‹å­ï¼ˆä¾‹å¦‚ï¼Œ5-10 ç·¨è¼¯åœ–åƒï¼‰å¿«é€Ÿé©æ‡‰æ–°å°ˆå®¶é¢¨æ ¼ã€‚è¨­è¨ˆå…ƒå­¸ç¿’æ–¹æ³•ï¼Œå…¶ä¸­ï¼š(1) åœ¨è¬é¡Œä¸Šè¨“ç·´çš„åŸºç¤æ¨¡å‹ç”¨ä½œåˆå§‹åŒ–ï¼Œ(2) ç”¨å°‘æ•¸å°ˆå®¶ç·¨è¼¯é€²è¡Œå¾®èª¿ï¼Œä½¿ç”¨ LoRA æˆ–é¡ä¼¼æ•ˆç‡é©æ‡‰ï¼Œ(3) è©•ä¼°è‡ªé©æ‡‰æ¨¡å‹æ˜¯å¦æ•æ‰å°ˆå®¶é¢¨æ ¼ã€‚æ­¤åœ¨å¯¦è¸ä¸Šæœ‰åƒ¹å€¼â€”æ”å½±å¸«é€šå¸¸å…·æœ‰ç¨ç‰¹é¢¨æ ¼ï¼Œå¿«é€Ÿè‡ªè¨‚å°‡ä½¿å€‹æ€§åŒ–ä¿®é£¾æˆç‚ºå¯èƒ½ã€‚æ­¤æ–¹æ³•é‚„å°‡æ¸¬è©¦æ“ä½œæ„ŸçŸ¥è¡¨ç¤ºçš„é€šç”¨æ€§ï¼šè‹¥å…¶ç¢ºå¯¦æ•æ‰åœ–åƒç·¨è¼¯çš„åŸºæœ¬åŸç†ï¼Œé©æ‡‰æ‡‰ç‚ºæ¨£æœ¬é«˜æ•ˆã€‚

</div>
